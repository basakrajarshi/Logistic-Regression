{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and SGD Homework \n",
    "***\n",
    "**Name**: $<$insert name here$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 9th**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "\n",
    "In this homework you'll implement stochastic gradient ascent for logistic regression and you'll apply it to the task of determining whether documents are talking about automobiles or motorcycles.\n",
    "\n",
    "<br>\n",
    "\n",
    "![autos_motorcycles](autos_motorcycles.jpg \"A car and a motorcycle\")\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "You should not use any libraries that implement any of the functionality of logistic regression for this assignment; logistic regression is implemented in Scikit-Learn, but you should do everything by hand now. You'll be able to use library implementations of logistic regression in the future.\n",
    "\n",
    "Here are the rules: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function or class API **do not** change it.\n",
    "- Do not change the location of the data or data directory.  Use only relative paths to access the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Problem 1: Loading and Exploring the Data\n",
    "***\n",
    "\n",
    "The `Example` class will be used to store the features and labels associated with a single training or test example.  The `read_data` function will read in the text data and split it into training and test sets.  \n",
    "\n",
    " Load the data and then do the following: \n",
    "- Report the number of words in the vocabulary \n",
    "- Explain how the code is creating features (i.e. what text model is being used). \n",
    "- Go into the raw text files in the data directory and figure out which label (0/1) refers to which class of document (automobiles or motorcycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update     1  TrnNLL  733.645  TstNLL   77.745  TrnA 0.498  TstA 0.534\n",
      "Update     6  TrnNLL  699.051  TstNLL   70.245  TrnA 0.614  TstA 0.612\n",
      "Update    11  TrnNLL  2021.949  TstNLL  195.824  TrnA 0.520  TstA 0.569\n",
      "Update    16  TrnNLL  1638.842  TstNLL  159.044  TrnA 0.539  TstA 0.595\n",
      "Update    21  TrnNLL  1112.565  TstNLL  111.909  TrnA 0.592  TstA 0.621\n",
      "Update    26  TrnNLL  876.943  TstNLL   92.343  TrnA 0.650  TstA 0.655\n",
      "Update    31  TrnNLL  520.123  TstNLL   55.389  TrnA 0.787  TstA 0.784\n",
      "Update    36  TrnNLL  506.739  TstNLL   53.554  TrnA 0.793  TstA 0.776\n",
      "Update    41  TrnNLL  480.412  TstNLL   51.261  TrnA 0.805  TstA 0.810\n",
      "Update    46  TrnNLL  504.111  TstNLL   49.096  TrnA 0.795  TstA 0.845\n",
      "Update    51  TrnNLL  475.415  TstNLL   48.127  TrnA 0.818  TstA 0.810\n",
      "Update    56  TrnNLL  515.194  TstNLL   53.856  TrnA 0.809  TstA 0.819\n",
      "Update    61  TrnNLL  458.333  TstNLL   48.827  TrnA 0.836  TstA 0.819\n",
      "Update    66  TrnNLL  899.422  TstNLL  105.021  TrnA 0.663  TstA 0.647\n",
      "Update    71  TrnNLL  729.381  TstNLL   85.950  TrnA 0.711  TstA 0.716\n",
      "Update    76  TrnNLL  570.147  TstNLL   60.703  TrnA 0.747  TstA 0.750\n",
      "Update    81  TrnNLL  508.576  TstNLL   53.742  TrnA 0.780  TstA 0.828\n",
      "Update    86  TrnNLL  478.821  TstNLL   48.982  TrnA 0.802  TstA 0.828\n",
      "Update    91  TrnNLL  469.668  TstNLL   48.974  TrnA 0.810  TstA 0.828\n",
      "Update    96  TrnNLL  408.337  TstNLL   43.188  TrnA 0.845  TstA 0.871\n",
      "Update   101  TrnNLL  411.817  TstNLL   46.655  TrnA 0.837  TstA 0.845\n",
      "Update   106  TrnNLL  427.384  TstNLL   50.801  TrnA 0.828  TstA 0.828\n",
      "Update   111  TrnNLL  421.423  TstNLL   47.123  TrnA 0.844  TstA 0.853\n",
      "Update   116  TrnNLL  483.833  TstNLL   51.785  TrnA 0.797  TstA 0.810\n",
      "Update   121  TrnNLL  488.694  TstNLL   52.361  TrnA 0.800  TstA 0.793\n",
      "Update   126  TrnNLL  473.244  TstNLL   53.088  TrnA 0.819  TstA 0.819\n",
      "Update   131  TrnNLL  470.997  TstNLL   51.911  TrnA 0.817  TstA 0.845\n",
      "Update   136  TrnNLL  488.385  TstNLL   53.965  TrnA 0.803  TstA 0.802\n",
      "Update   141  TrnNLL  501.137  TstNLL   56.006  TrnA 0.779  TstA 0.767\n",
      "Update   146  TrnNLL  517.492  TstNLL   57.828  TrnA 0.757  TstA 0.707\n",
      "Update   151  TrnNLL  510.146  TstNLL   57.417  TrnA 0.755  TstA 0.724\n",
      "Update   156  TrnNLL  460.729  TstNLL   53.230  TrnA 0.834  TstA 0.819\n",
      "Update   161  TrnNLL  466.572  TstNLL   54.279  TrnA 0.848  TstA 0.793\n",
      "Update   166  TrnNLL  455.732  TstNLL   54.432  TrnA 0.859  TstA 0.828\n",
      "Update   171  TrnNLL  448.361  TstNLL   53.903  TrnA 0.871  TstA 0.853\n",
      "Update   176  TrnNLL  474.716  TstNLL   60.227  TrnA 0.828  TstA 0.793\n",
      "Update   181  TrnNLL  461.112  TstNLL   58.383  TrnA 0.836  TstA 0.784\n",
      "Update   186  TrnNLL  472.808  TstNLL   57.878  TrnA 0.816  TstA 0.741\n",
      "Update   191  TrnNLL  455.905  TstNLL   54.324  TrnA 0.849  TstA 0.802\n",
      "Update   196  TrnNLL  479.086  TstNLL   64.112  TrnA 0.776  TstA 0.776\n",
      "Update   201  TrnNLL  434.433  TstNLL   59.863  TrnA 0.825  TstA 0.845\n",
      "Update   206  TrnNLL  452.118  TstNLL   61.429  TrnA 0.806  TstA 0.819\n",
      "Update   211  TrnNLL  429.738  TstNLL   58.267  TrnA 0.862  TstA 0.845\n",
      "Update   216  TrnNLL  437.371  TstNLL   55.977  TrnA 0.863  TstA 0.784\n",
      "Update   221  TrnNLL  442.285  TstNLL   57.424  TrnA 0.875  TstA 0.810\n",
      "Update   226  TrnNLL  409.520  TstNLL   56.296  TrnA 0.888  TstA 0.862\n",
      "Update   231  TrnNLL  437.105  TstNLL   63.076  TrnA 0.845  TstA 0.750\n",
      "Update   236  TrnNLL  436.447  TstNLL   62.468  TrnA 0.871  TstA 0.741\n",
      "Update   241  TrnNLL  436.693  TstNLL   60.934  TrnA 0.841  TstA 0.750\n",
      "Update   246  TrnNLL  440.569  TstNLL   59.668  TrnA 0.852  TstA 0.776\n",
      "Update   251  TrnNLL  440.156  TstNLL   58.970  TrnA 0.862  TstA 0.784\n",
      "Update   256  TrnNLL  412.037  TstNLL   51.570  TrnA 0.877  TstA 0.828\n",
      "Update   261  TrnNLL  423.616  TstNLL   52.146  TrnA 0.901  TstA 0.862\n",
      "Update   266  TrnNLL  423.452  TstNLL   51.958  TrnA 0.876  TstA 0.793\n",
      "Update   271  TrnNLL  469.362  TstNLL   60.021  TrnA 0.812  TstA 0.741\n",
      "Update   276  TrnNLL  408.733  TstNLL   50.380  TrnA 0.882  TstA 0.810\n",
      "Update   281  TrnNLL  419.403  TstNLL   50.884  TrnA 0.880  TstA 0.810\n",
      "Update   286  TrnNLL  410.769  TstNLL   49.241  TrnA 0.889  TstA 0.819\n",
      "Update   291  TrnNLL  406.107  TstNLL   47.704  TrnA 0.895  TstA 0.845\n",
      "Update   296  TrnNLL  413.639  TstNLL   50.423  TrnA 0.866  TstA 0.836\n",
      "Update   301  TrnNLL  401.503  TstNLL   48.735  TrnA 0.911  TstA 0.897\n",
      "Update   306  TrnNLL  437.541  TstNLL   53.551  TrnA 0.829  TstA 0.784\n",
      "Update   311  TrnNLL  410.208  TstNLL   49.612  TrnA 0.902  TstA 0.862\n",
      "Update   316  TrnNLL  411.237  TstNLL   49.838  TrnA 0.902  TstA 0.836\n",
      "Update   321  TrnNLL  502.142  TstNLL   58.811  TrnA 0.794  TstA 0.750\n",
      "Update   326  TrnNLL  479.022  TstNLL   56.747  TrnA 0.844  TstA 0.819\n",
      "Update   331  TrnNLL  464.204  TstNLL   56.431  TrnA 0.842  TstA 0.810\n",
      "Update   336  TrnNLL  485.425  TstNLL   57.874  TrnA 0.797  TstA 0.716\n",
      "Update   341  TrnNLL  488.444  TstNLL   58.043  TrnA 0.794  TstA 0.741\n",
      "Update   346  TrnNLL  443.870  TstNLL   53.829  TrnA 0.873  TstA 0.810\n",
      "Update   351  TrnNLL  460.908  TstNLL   58.832  TrnA 0.857  TstA 0.793\n",
      "Update   356  TrnNLL  430.579  TstNLL   54.223  TrnA 0.872  TstA 0.776\n",
      "Update   361  TrnNLL  467.534  TstNLL   56.308  TrnA 0.809  TstA 0.759\n",
      "Update   366  TrnNLL  449.114  TstNLL   54.656  TrnA 0.858  TstA 0.810\n",
      "Update   371  TrnNLL  451.966  TstNLL   54.461  TrnA 0.854  TstA 0.784\n",
      "Update   376  TrnNLL  448.296  TstNLL   55.195  TrnA 0.862  TstA 0.845\n",
      "Update   381  TrnNLL  425.895  TstNLL   50.805  TrnA 0.892  TstA 0.871\n",
      "Update   386  TrnNLL  449.903  TstNLL   52.638  TrnA 0.834  TstA 0.862\n",
      "Update   391  TrnNLL  418.158  TstNLL   50.427  TrnA 0.899  TstA 0.853\n",
      "Update   396  TrnNLL  485.950  TstNLL   57.555  TrnA 0.826  TstA 0.810\n",
      "Update   401  TrnNLL  434.760  TstNLL   52.952  TrnA 0.886  TstA 0.836\n",
      "Update   406  TrnNLL  452.847  TstNLL   55.183  TrnA 0.880  TstA 0.845\n",
      "Update   411  TrnNLL  454.567  TstNLL   56.055  TrnA 0.876  TstA 0.862\n",
      "Update   416  TrnNLL  509.312  TstNLL   61.377  TrnA 0.794  TstA 0.759\n",
      "Update   421  TrnNLL  572.119  TstNLL   65.862  TrnA 0.733  TstA 0.629\n",
      "Update   426  TrnNLL  490.331  TstNLL   56.434  TrnA 0.791  TstA 0.733\n",
      "Update   431  TrnNLL  481.006  TstNLL   55.613  TrnA 0.789  TstA 0.733\n",
      "Update   436  TrnNLL  480.526  TstNLL   55.822  TrnA 0.798  TstA 0.724\n",
      "Update   441  TrnNLL  445.553  TstNLL   53.545  TrnA 0.841  TstA 0.810\n",
      "Update   446  TrnNLL  422.911  TstNLL   51.402  TrnA 0.885  TstA 0.871\n",
      "Update   451  TrnNLL  430.293  TstNLL   53.238  TrnA 0.879  TstA 0.862\n",
      "Update   456  TrnNLL  473.274  TstNLL   58.668  TrnA 0.789  TstA 0.776\n",
      "Update   461  TrnNLL  494.838  TstNLL   61.542  TrnA 0.755  TstA 0.750\n",
      "Update   466  TrnNLL  432.811  TstNLL   52.694  TrnA 0.882  TstA 0.853\n",
      "Update   471  TrnNLL  758.783  TstNLL   80.749  TrnA 0.644  TstA 0.621\n",
      "Update   476  TrnNLL  661.734  TstNLL   69.742  TrnA 0.681  TstA 0.716\n",
      "Update   481  TrnNLL  505.009  TstNLL   54.627  TrnA 0.796  TstA 0.828\n",
      "Update   486  TrnNLL  475.248  TstNLL   51.501  TrnA 0.811  TstA 0.862\n",
      "Update   491  TrnNLL  530.974  TstNLL   58.105  TrnA 0.750  TstA 0.759\n",
      "Update   496  TrnNLL  463.432  TstNLL   51.772  TrnA 0.831  TstA 0.862\n",
      "Update   501  TrnNLL  458.060  TstNLL   51.567  TrnA 0.833  TstA 0.810\n",
      "Update   506  TrnNLL  425.111  TstNLL   49.378  TrnA 0.865  TstA 0.879\n",
      "Update   511  TrnNLL  444.724  TstNLL   53.353  TrnA 0.854  TstA 0.853\n",
      "Update   516  TrnNLL  432.457  TstNLL   51.669  TrnA 0.876  TstA 0.888\n",
      "Update   521  TrnNLL  446.708  TstNLL   53.673  TrnA 0.857  TstA 0.871\n",
      "Update   526  TrnNLL  450.961  TstNLL   53.327  TrnA 0.862  TstA 0.836\n",
      "Update   531  TrnNLL  431.280  TstNLL   51.568  TrnA 0.875  TstA 0.862\n",
      "Update   536  TrnNLL  471.854  TstNLL   54.385  TrnA 0.813  TstA 0.836\n",
      "Update   541  TrnNLL  451.457  TstNLL   51.846  TrnA 0.843  TstA 0.836\n",
      "Update   546  TrnNLL  425.032  TstNLL   50.460  TrnA 0.863  TstA 0.871\n",
      "Update   551  TrnNLL  438.860  TstNLL   50.788  TrnA 0.869  TstA 0.853\n",
      "Update   556  TrnNLL  433.461  TstNLL   49.082  TrnA 0.869  TstA 0.871\n",
      "Update   561  TrnNLL  706.963  TstNLL   75.142  TrnA 0.635  TstA 0.629\n",
      "Update   566  TrnNLL  747.400  TstNLL   78.422  TrnA 0.592  TstA 0.595\n",
      "Update   571  TrnNLL  498.380  TstNLL   53.141  TrnA 0.796  TstA 0.802\n",
      "Update   576  TrnNLL  509.974  TstNLL   54.064  TrnA 0.771  TstA 0.802\n",
      "Update   581  TrnNLL  443.179  TstNLL   46.836  TrnA 0.836  TstA 0.853\n",
      "Update   586  TrnNLL  456.265  TstNLL   48.907  TrnA 0.830  TstA 0.819\n",
      "Update   591  TrnNLL  453.693  TstNLL   49.247  TrnA 0.825  TstA 0.871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update   596  TrnNLL  474.611  TstNLL   53.098  TrnA 0.797  TstA 0.828\n",
      "Update   601  TrnNLL  462.975  TstNLL   52.219  TrnA 0.812  TstA 0.845\n",
      "Update   606  TrnNLL  427.664  TstNLL   48.729  TrnA 0.881  TstA 0.897\n",
      "Update   611  TrnNLL  434.259  TstNLL   50.950  TrnA 0.862  TstA 0.862\n",
      "Update   616  TrnNLL  443.903  TstNLL   51.863  TrnA 0.853  TstA 0.853\n",
      "Update   621  TrnNLL  432.772  TstNLL   52.684  TrnA 0.854  TstA 0.836\n",
      "Update   626  TrnNLL  463.016  TstNLL   56.381  TrnA 0.782  TstA 0.759\n",
      "Update   631  TrnNLL  437.428  TstNLL   53.428  TrnA 0.825  TstA 0.819\n",
      "Update   636  TrnNLL  419.392  TstNLL   50.909  TrnA 0.874  TstA 0.845\n",
      "Update   641  TrnNLL  454.512  TstNLL   55.360  TrnA 0.833  TstA 0.810\n",
      "Update   646  TrnNLL  446.257  TstNLL   53.955  TrnA 0.850  TstA 0.845\n",
      "Update   651  TrnNLL  482.824  TstNLL   57.940  TrnA 0.788  TstA 0.759\n",
      "Update   656  TrnNLL  515.301  TstNLL   61.399  TrnA 0.754  TstA 0.724\n",
      "Update   661  TrnNLL  441.511  TstNLL   54.127  TrnA 0.880  TstA 0.819\n",
      "Update   666  TrnNLL  441.874  TstNLL   54.552  TrnA 0.876  TstA 0.828\n",
      "Update   671  TrnNLL  432.237  TstNLL   53.811  TrnA 0.891  TstA 0.828\n",
      "Update   676  TrnNLL  459.018  TstNLL   54.937  TrnA 0.852  TstA 0.776\n",
      "Update   681  TrnNLL  521.147  TstNLL   61.401  TrnA 0.757  TstA 0.707\n",
      "Update   686  TrnNLL  489.003  TstNLL   55.427  TrnA 0.836  TstA 0.836\n",
      "Update   691  TrnNLL  460.617  TstNLL   50.793  TrnA 0.855  TstA 0.828\n",
      "Update   696  TrnNLL  459.476  TstNLL   53.445  TrnA 0.847  TstA 0.836\n",
      "Update   701  TrnNLL  499.932  TstNLL   55.913  TrnA 0.786  TstA 0.802\n",
      "Update   706  TrnNLL  531.789  TstNLL   60.138  TrnA 0.796  TstA 0.802\n",
      "Update   711  TrnNLL  546.222  TstNLL   59.321  TrnA 0.779  TstA 0.793\n",
      "Update   716  TrnNLL  506.137  TstNLL   56.507  TrnA 0.825  TstA 0.819\n",
      "Update   721  TrnNLL  489.335  TstNLL   54.149  TrnA 0.836  TstA 0.845\n",
      "Update   726  TrnNLL  479.879  TstNLL   53.353  TrnA 0.849  TstA 0.836\n",
      "Update   731  TrnNLL  461.108  TstNLL   52.783  TrnA 0.874  TstA 0.836\n",
      "Update   736  TrnNLL  455.582  TstNLL   53.090  TrnA 0.875  TstA 0.802\n",
      "Update   741  TrnNLL  605.969  TstNLL   70.156  TrnA 0.696  TstA 0.664\n",
      "Update   746  TrnNLL  599.702  TstNLL   70.600  TrnA 0.686  TstA 0.655\n",
      "Update   751  TrnNLL  463.663  TstNLL   58.119  TrnA 0.805  TstA 0.759\n",
      "Update   756  TrnNLL  469.589  TstNLL   57.155  TrnA 0.836  TstA 0.784\n",
      "Update   761  TrnNLL  466.607  TstNLL   55.620  TrnA 0.824  TstA 0.853\n",
      "Update   766  TrnNLL  482.793  TstNLL   57.194  TrnA 0.817  TstA 0.810\n",
      "Update   771  TrnNLL  514.892  TstNLL   63.425  TrnA 0.768  TstA 0.741\n",
      "Update   776  TrnNLL  517.485  TstNLL   63.471  TrnA 0.772  TstA 0.716\n",
      "Update   781  TrnNLL  548.606  TstNLL   66.725  TrnA 0.724  TstA 0.672\n",
      "Update   786  TrnNLL  511.207  TstNLL   63.589  TrnA 0.773  TstA 0.741\n",
      "Update   791  TrnNLL  468.846  TstNLL   58.149  TrnA 0.860  TstA 0.776\n",
      "Update   796  TrnNLL  566.353  TstNLL   68.647  TrnA 0.724  TstA 0.681\n",
      "Update   801  TrnNLL  525.073  TstNLL   66.112  TrnA 0.765  TstA 0.759\n",
      "Update   806  TrnNLL  486.142  TstNLL   60.274  TrnA 0.814  TstA 0.741\n",
      "Update   811  TrnNLL  485.782  TstNLL   59.881  TrnA 0.823  TstA 0.741\n",
      "Update   816  TrnNLL  469.725  TstNLL   57.957  TrnA 0.844  TstA 0.784\n",
      "Update   821  TrnNLL  489.260  TstNLL   58.307  TrnA 0.819  TstA 0.741\n",
      "Update   826  TrnNLL  497.361  TstNLL   60.896  TrnA 0.797  TstA 0.750\n",
      "Update   831  TrnNLL  543.881  TstNLL   64.027  TrnA 0.724  TstA 0.698\n",
      "Update   836  TrnNLL  494.966  TstNLL   57.834  TrnA 0.809  TstA 0.784\n",
      "Update   841  TrnNLL  492.106  TstNLL   58.622  TrnA 0.781  TstA 0.784\n",
      "Update   846  TrnNLL  491.839  TstNLL   58.238  TrnA 0.777  TstA 0.767\n",
      "Update   851  TrnNLL  473.660  TstNLL   58.303  TrnA 0.807  TstA 0.767\n",
      "Update   856  TrnNLL  462.138  TstNLL   57.772  TrnA 0.808  TstA 0.759\n",
      "Update   861  TrnNLL  433.500  TstNLL   53.465  TrnA 0.861  TstA 0.802\n",
      "Update   866  TrnNLL  435.373  TstNLL   53.493  TrnA 0.855  TstA 0.810\n",
      "Update   871  TrnNLL  444.248  TstNLL   54.427  TrnA 0.851  TstA 0.784\n",
      "Update   876  TrnNLL  442.224  TstNLL   54.409  TrnA 0.850  TstA 0.836\n",
      "Update   881  TrnNLL  467.739  TstNLL   55.939  TrnA 0.836  TstA 0.810\n",
      "Update   886  TrnNLL  436.925  TstNLL   50.447  TrnA 0.871  TstA 0.879\n",
      "Update   891  TrnNLL  445.384  TstNLL   51.466  TrnA 0.834  TstA 0.845\n",
      "Update   896  TrnNLL  467.752  TstNLL   53.160  TrnA 0.793  TstA 0.810\n",
      "Update   901  TrnNLL  451.208  TstNLL   49.954  TrnA 0.826  TstA 0.853\n",
      "Update   906  TrnNLL  456.842  TstNLL   51.186  TrnA 0.835  TstA 0.845\n",
      "Update   911  TrnNLL  448.506  TstNLL   50.032  TrnA 0.860  TstA 0.862\n",
      "Update   916  TrnNLL  447.293  TstNLL   50.468  TrnA 0.851  TstA 0.836\n",
      "Update   921  TrnNLL  433.635  TstNLL   49.606  TrnA 0.887  TstA 0.871\n",
      "Update   926  TrnNLL  438.041  TstNLL   50.270  TrnA 0.877  TstA 0.888\n",
      "Update   931  TrnNLL  419.935  TstNLL   48.225  TrnA 0.870  TstA 0.871\n",
      "Update   936  TrnNLL  424.824  TstNLL   49.748  TrnA 0.879  TstA 0.828\n",
      "Update   941  TrnNLL  419.675  TstNLL   49.952  TrnA 0.888  TstA 0.845\n",
      "Update   946  TrnNLL  437.738  TstNLL   51.310  TrnA 0.865  TstA 0.836\n",
      "Update   951  TrnNLL  432.581  TstNLL   50.183  TrnA 0.875  TstA 0.853\n",
      "Update   956  TrnNLL  436.834  TstNLL   49.525  TrnA 0.874  TstA 0.879\n",
      "Update   961  TrnNLL  439.053  TstNLL   50.210  TrnA 0.874  TstA 0.845\n",
      "Update   966  TrnNLL  437.541  TstNLL   52.046  TrnA 0.878  TstA 0.845\n",
      "Update   971  TrnNLL  432.196  TstNLL   50.903  TrnA 0.888  TstA 0.871\n",
      "Update   976  TrnNLL  470.510  TstNLL   54.202  TrnA 0.828  TstA 0.793\n",
      "Update   981  TrnNLL  492.034  TstNLL   56.191  TrnA 0.794  TstA 0.776\n",
      "Update   986  TrnNLL  491.716  TstNLL   56.637  TrnA 0.802  TstA 0.802\n",
      "Update   991  TrnNLL  450.252  TstNLL   53.663  TrnA 0.880  TstA 0.828\n",
      "Update   996  TrnNLL  438.626  TstNLL   52.021  TrnA 0.889  TstA 0.853\n",
      "Update  1001  TrnNLL  438.907  TstNLL   52.025  TrnA 0.883  TstA 0.819\n",
      "Update  1006  TrnNLL  451.692  TstNLL   52.502  TrnA 0.862  TstA 0.819\n",
      "Update  1011  TrnNLL  441.094  TstNLL   51.489  TrnA 0.894  TstA 0.845\n",
      "Update  1016  TrnNLL  436.246  TstNLL   51.697  TrnA 0.885  TstA 0.819\n",
      "Update  1021  TrnNLL  538.291  TstNLL   61.350  TrnA 0.742  TstA 0.647\n",
      "Update  1026  TrnNLL  526.233  TstNLL   62.236  TrnA 0.749  TstA 0.664\n",
      "Update  1031  TrnNLL  477.990  TstNLL   56.908  TrnA 0.811  TstA 0.776\n",
      "Update  1036  TrnNLL  424.705  TstNLL   52.439  TrnA 0.883  TstA 0.836\n",
      "Update  1041  TrnNLL  428.847  TstNLL   53.338  TrnA 0.869  TstA 0.871\n",
      "Update  1046  TrnNLL  431.971  TstNLL   53.324  TrnA 0.861  TstA 0.871\n",
      "Update  1051  TrnNLL  493.753  TstNLL   56.811  TrnA 0.804  TstA 0.776\n",
      "Update  1056  TrnNLL  655.114  TstNLL   78.045  TrnA 0.654  TstA 0.603\n",
      "Update  1061  TrnNLL  589.072  TstNLL   70.711  TrnA 0.707  TstA 0.647\n",
      "Update  1066  TrnNLL  517.433  TstNLL   62.681  TrnA 0.789  TstA 0.716\n",
      "Update  1071  TrnNLL  454.101  TstNLL   55.413  TrnA 0.849  TstA 0.802\n",
      "Update  1076  TrnNLL  456.004  TstNLL   54.282  TrnA 0.849  TstA 0.819\n",
      "Motorcycle: \n",
      "['ca', 'michael', 'list', 'two', 'race', 'sun', '11', 'ken', 'bike', 'lock']\n",
      "Car: \n",
      "['car', 'people', 'see', 'cars', 'unit', 'make', 'months', 'price', 'backing', 'williams']\n",
      "Bad: \n",
      "['helping', 'distributor', 'respectful', 'absence', 'infamous', 'highest', 'prone', 'expound', 'goal', 'burst']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "kSEED = 1735\n",
    "kBIAS = \"BIAS_CONSTANT\"\n",
    "\n",
    "np.random.seed(kSEED)\n",
    "\n",
    "class Example:\n",
    "    \"\"\"\n",
    "    Class to represent a document example\n",
    "    \"\"\"\n",
    "    def __init__(self, label, words, vocab):\n",
    "        \"\"\"\n",
    "        Create a new example\n",
    "\n",
    "        :param label: The label (0 / 1) of the example\n",
    "        :param words: The words in a list of \"word:count\" format\n",
    "        :param vocab: The vocabulary to use as features (list)\n",
    "        \"\"\"\n",
    "        self.nonzero = {}\n",
    "        self.y = label\n",
    "        self.x = np.zeros(len(vocab))\n",
    "        for word, count in [x.split(\":\") for x in words]:\n",
    "            if word in vocab:\n",
    "                assert word != kBIAS, \"Bias can't actually appear in document\"\n",
    "                self.x[vocab.index(word)] += float(count)\n",
    "                self.nonzero[vocab.index(word)] = word\n",
    "        self.x[0] = 1\n",
    "\n",
    "def read_dataset(positive, negative, vocab, train_frac=0.9):\n",
    "    \"\"\"\n",
    "    Reads in a text dataset with a given vocabulary\n",
    "\n",
    "    :param positive: Positive examples\n",
    "    :param negative: Negative examples\n",
    "    :param vocab: A list of vocabulary words\n",
    "    :param test_frac: How much of the data should be reserved for test\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = [x.split(\"\\t\")[0] for x in open(vocab, 'r') if '\\t' in x]\n",
    "    assert vocab[0] == kBIAS, \\\n",
    "        \"First vocab word must be bias term (was %s)\" % vocab[0]\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for label, input in [(1, positive), (0, negative)]:\n",
    "        for line in open(input):\n",
    "            ex = Example(label, line.split(), vocab)\n",
    "            if np.random.random() <= train_frac:\n",
    "                train_set.append(ex)\n",
    "            else:\n",
    "                test_set.append(ex)\n",
    "\n",
    "    # Shuffle the data \n",
    "    np.random.shuffle(train_set)\n",
    "    np.random.shuffle(test_set)\n",
    "\n",
    "    return train_set, test_set, vocab\n",
    "\n",
    "\n",
    "pos_fname = \"positive\"\n",
    "neg_fname = \"negative\"\n",
    "voc_fname = \"vocab\"\n",
    "train_set, test_set, vocab = read_dataset(pos_fname, neg_fname, voc_fname)\n",
    "# print(\"No of training examples: \")\n",
    "# print(len(vocab))\n",
    "\n",
    "# features are being created using bag of word model. Each word that appears \n",
    "# in positive or negative example basically \n",
    "\n",
    "class LogReg:\n",
    "    def __init__(self, train_set, test_set, lam, eta=0.1):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "\n",
    "        :param train_set: A set of training examples\n",
    "        :param test_set: A set of test examples \n",
    "        :param lam: Regularization parameter\n",
    "        :param eta: The learning rate to use \n",
    "        \"\"\"\n",
    "        \n",
    "        # Store training and test sets \n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set \n",
    "        \n",
    "        # Initialize vector of weights to zero  \n",
    "        self.w = np.zeros_like(train_set[0].x)\n",
    "        \n",
    "        # Store regularization parameter and eta function \n",
    "        self.lam = lam\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Create dictionary for lazy-sparse regularization\n",
    "        self.last_update = defaultdict(int)\n",
    "\n",
    "        # Make sure regularization parameter is not negative \n",
    "        assert self.lam>= 0, \"Regularization parameter must be non-negative\"\n",
    "        \n",
    "        # Empty lists to store NLL and accuracy on train and test sets \n",
    "        self.train_nll = []\n",
    "        self.test_nll = []\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        self.iterationcount = [] \n",
    "      \n",
    "    def sigmoid(self,score, threshold=20.0):\n",
    "        \"\"\"\n",
    "        Prevent overflow of exp by capping activation at 20.\n",
    "        You do not need to change this function. \n",
    "\n",
    "        :param score: A real valued number to convert into a number between 0 and 1\n",
    "        \"\"\"\n",
    "\n",
    "        # if score > threshold, cap value at score \n",
    "        if abs(score) > threshold:\n",
    "            score = threshold * np.sign(score)\n",
    "\n",
    "        return 1.0 / (1.0 + np.exp(-score)) \n",
    "\n",
    "    def compute_progress(self, examples):\n",
    "        \"\"\"\n",
    "        Given a set of examples, compute the NLL and accuracy\n",
    "        You shouldn't need to change this function. \n",
    "\n",
    "        :param examples: The dataset to score\n",
    "        :return: A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "\n",
    "        NLL = 0.0\n",
    "        num_correct = 0\n",
    "        for ex in examples:\n",
    "            # compute prob prediction\n",
    "            p = self.sigmoid(self.w.dot(ex.x))\n",
    "            # update negative log likelihood\n",
    "            NLL = NLL - np.log(p) if ex.y==1 else NLL - np.log(1.0-p)\n",
    "            # update number correct \n",
    "            num_correct += 1 if np.floor(p+.5)==ex.y else 0\n",
    "\n",
    "        return NLL, float(num_correct) / float(len(examples))\n",
    "    \n",
    "    def train(self, num_epochs=1, isVerbose=False, report_step=5):\n",
    "        \"\"\"\n",
    "        Train the logistic regression classifier on the training data \n",
    "\n",
    "        :param num_epochs: number of full passes over data to perform \n",
    "        :param isVerbose: boolean indicating whether to print progress\n",
    "        :param report_step: how many iterations between recording progress\n",
    "        \"\"\"\n",
    "        iteration = 0\n",
    "        # Perform an epoch \n",
    "        for pp in range(num_epochs):\n",
    "            # shuffle the data  \n",
    "            np.random.shuffle(self.train_set)\n",
    "            # loop over each training example\n",
    "            for ex in self.train_set:\n",
    "                # perform SGD update of weights\n",
    "                iteration += 1\n",
    "                self.sgd_update(ex, iteration)\n",
    "                # record progress \n",
    "                if iteration % report_step == 1:\n",
    "                    train_nll, train_acc = self.compute_progress(self.train_set)\n",
    "                    test_nll, test_acc = self.compute_progress(self.test_set)\n",
    "                    self.train_nll.append(train_nll)\n",
    "                    self.test_nll.append(test_nll)\n",
    "                    self.train_acc.append(train_acc)\n",
    "                    self.test_acc.append(test_acc)\n",
    "                    self.iterationcount.append(iteration)\n",
    "                    if isVerbose:\n",
    "                        print(\"Update {: 5d}  TrnNLL {: 8.3f}  TstNLL {: 8.3f}  TrnA {:.3f}  TstA {:.3f}\"\n",
    "                             .format(iteration, train_nll, test_nll, train_acc, test_acc))\n",
    "    \n",
    "    def testdata(self, num_epochs=1, isVerbose=False, report_step=5):\n",
    "        \"\"\"\n",
    "        Train the logistic regression classifier on the training data \n",
    "\n",
    "        :param num_epochs: number of full passes over data to perform \n",
    "        :param isVerbose: boolean indicating whether to print progress\n",
    "        :param report_step: how many iterations between recording progress\n",
    "        \"\"\"\n",
    "        iteration = 0\n",
    "        # Perform an epoch \n",
    "        for pp in range(num_epochs):\n",
    "            # shuffle the data  \n",
    "            np.random.shuffle(self.test_set)\n",
    "            # loop over each training example\n",
    "            for ex in self.test_set:\n",
    "                # perform SGD update of weights\n",
    "                iteration += 1\n",
    "                self.sgd_update(ex, iteration)\n",
    "                # record progress \n",
    "                if iteration % report_step == 1:\n",
    "                    train_nll, train_acc = self.compute_progress(self.train_set)\n",
    "                    test_nll, test_acc = self.compute_progress(self.test_set)\n",
    "                    self.train_nll.append(train_nll)\n",
    "                    self.test_nll.append(test_nll)\n",
    "                    self.train_acc.append(train_acc)\n",
    "                    self.test_acc.append(test_acc)\n",
    "                    self.iterationcount.append(iteration)\n",
    "                    if isVerbose:\n",
    "                        print(\"Update {: 5d}  TrnNLL {: 8.3f}  TstNLL {: 8.3f}  TrnA {:.3f}  TstA {:.3f}\"\n",
    "                             .format(iteration, train_nll, test_nll, train_acc, test_acc))\n",
    "                \n",
    "                \n",
    "    \n",
    "    def sgd_update(self, train_example, iteration, use_tfidf=False):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update to improve the log likelihood.\n",
    "        :param train_example: The example to take the gradient with respect to\n",
    "        :param iteration: The current iteration (an integer)\n",
    "        :param use_tfidf: A boolean to switch between the raw data and the tfidf representation\n",
    "        :return: Return the new value of the regression coefficients\n",
    "        \"\"\"\n",
    "        \n",
    "        idx = list(train_example.nonzero.keys())\n",
    "        idx.append(0)\n",
    "        \n",
    "        x    =  train_example.x\n",
    "        y    =  train_example.y\n",
    "\n",
    "        ebiasx = math.exp(self.w[idx].dot(x[idx]))\n",
    "        p = ebiasx/float(1+ebiasx)\n",
    "        self.w[idx] = self.w[idx] + ((self.eta) * (y - p) * x[idx])\n",
    "        biasunreg = self.w[0] \n",
    "        \n",
    "        for i in range(0, len(self.w)):\n",
    "            self.last_update[i] += 1\n",
    "        regular = np.power((1 - (2 * self.eta * self.lam)),((list(self.last_update.values()))))\n",
    "        self.w[idx] = self.w[idx] * regular[idx]\n",
    "        \n",
    "        self.w[0] = biasunreg\n",
    "        for i in idx:\n",
    "            self.last_update[i]=0\n",
    "\n",
    "    def goodbbwords(self):\n",
    "        return np.argsort(self.w)[-10:]\n",
    "    def goodhwords(self):\n",
    "        return np.argsort(self.w)[:10]\n",
    "    def badwords(self):\n",
    "        return np.argsort(abs(self.w))[:10]\n",
    "\n",
    "def results(lr):\n",
    "    print (\"Motorcycle: \")\n",
    "    baseball = lr.goodbbwords()\n",
    "    print([vocab[i] for i in baseball])\n",
    "    print(\"Car: \")\n",
    "    hockey = lr.goodhwords()\n",
    "    print([vocab[i] for i in hockey])\n",
    "    print (\"Bad: \")\n",
    "    bad = lr.badwords()\n",
    "    print([vocab[i] for i in bad])\n",
    "\n",
    "# %run -i tests.py \"part A\"\n",
    "# %run -i tests.py \"part B\"\n",
    "\n",
    "'''lamd = np.arange(0.001, 0.2, 0.005)\n",
    "for lam in lamd:\n",
    "    lr = LogReg(train_set, test_set, lam, eta=0.1)\n",
    "    lr.test(isVerbose=False)\n",
    "    plt.title(lam)\n",
    "    plt.plot(lr.iterationcount,lr.test_acc)\n",
    "    plt.show()\n",
    "# Above plots imply lam = 0.101, no overfitting, in some cases accuracy goes to 1, implying overfitting\n",
    "\n",
    "\n",
    "etar = np.arange(0.1, 1, 0.1)\n",
    "for et in etar:\n",
    "    lr = LogReg(train_set, test_set, lam=0.101, eta=et)\n",
    "    lr.test(isVerbose=False)\n",
    "    plt.title(et)\n",
    "    plt.plot(lr.iterationcount,lr.test_acc)\n",
    "    plt.show()'''\n",
    "# from the second set of plots eta = 0.1\n",
    "\n",
    "# Sort the weights in ascending order to get the best predictors for Positive class\n",
    "# and in descending order for negative class\n",
    "# The bad words are the ones with probabilities closest to zero, in ther words they are \n",
    "#equivalent to not being there in the training/test set\n",
    "lr = LogReg(train_set, test_set, lam=0.151, eta=0.1)\n",
    "lr.train(isVerbose=True)\n",
    "results(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_unreg (__main__.TestLogReg) ... ok\n",
      "test_learnrate (__main__.TestLogReg) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests.py \"part A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_reg (__main__.TestLogReg) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests.py \"part B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Implementing SGD with Lazy Sparse Regularization\n",
    "***\n",
    "\n",
    "We've given you a class `LogReg` below which will train a logistic regression classifier to predict whether a document is talking about automobiles or motorcycles. \n",
    "\n",
    "**Part A**: In this problem you will modify the `sgd_update` function to perform **unregularized** stochastic gradient descent updates of the weights. Note that you should only update the weights for **non-zero** features, i.e. weights associated with words that appear in the current training example. The code below this cell demonstrates how to instantiate the class and train the classifier.   \n",
    "\n",
    "We've also given you unit tests in the next cell based on the simple example worked out in  the Lecture 4 in-class notebook.  At first your code will fail both of them. When your code is working you should pass tests called `test_unreg` and `test_learnrate`.  Do not move on to **Part A** until your code passes both of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit tests are located in the script `tests.py` in this directory.  Execute the following cell to call the script and run the tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: After your unregularized updates are working, modify the `sgd_update` function again to perform regularized updates using **Lazy Sparse Regularization**. Note that you should not regularize the bias weight. See the Lecture 4 in-class notebook for a refresher on LSR. **Note**: After implementing LSR, your code should still pass the unit tests for **Part A** when `lam = 0`. \n",
    "\n",
    "We've given you a third unit test in the next cell called `test_reg` based on the simple example of LSR worked out in  the Lecture 4 in-class notebook.  Do not move on to **Problem 3** until your code passes the test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 3: Hyperparameter Tuning \n",
    "***\n",
    "\n",
    "**Part A**: Perform a systematic study of the effect of the regularization parameter on the accuracy of your classifier on the test set.  Which choice of `lam` seems to do the best?  Justify your conclusion with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: For the value of `lam` chosen in **Part A** perform a systematic study of the choice of learning rate on the speed of convergence SGD.  Which learning rate seems to give the fastest convergence?  Justify your conclusion with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 4: Identifying Predictive and Non-Predictive Words \n",
    "***\n",
    "\n",
    "**Part A**: Find the top 10 words that are the best predictors for each class.  Explain mathematically how you identified them and show any code that you used to find them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Find the 10 words that are the worst predictors for class.  Explain mathematically how you identified them and show any code that you used to find them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
