{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and SGD Homework \n",
    "***\n",
    "**Name**: $<$insert name here$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 9th**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "\n",
    "In this homework you'll implement stochastic gradient ascent for logistic regression and you'll apply it to the task of determining whether documents are talking about automobiles or motorcycles.\n",
    "\n",
    "<br>\n",
    "\n",
    "![autos_motorcycles](autos_motorcycles.jpg \"A car and a motorcycle\")\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "You should not use any libraries that implement any of the functionality of logistic regression for this assignment; logistic regression is implemented in Scikit-Learn, but you should do everything by hand now. You'll be able to use library implementations of logistic regression in the future.\n",
    "\n",
    "Here are the rules: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function or class API **do not** change it.\n",
    "- Do not change the location of the data or data directory.  Use only relative paths to access the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Problem 1: Loading and Exploring the Data\n",
    "***\n",
    "\n",
    "The `Example` class will be used to store the features and labels associated with a single training or test example.  The `read_data` function will read in the text data and split it into training and test sets.  \n",
    "\n",
    " Load the data and then do the following: \n",
    "- Report the number of words in the vocabulary \n",
    "- Explain how the code is creating features (i.e. what text model is being used). \n",
    "- Go into the raw text files in the data directory and figure out which label (0/1) refers to which class of document (automobiles or motorcycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5327\n",
      "Update     1  TrnNLL  733.385  TstNLL   77.689  TrnA 0.498  TstA 0.534\n",
      "Update     6  TrnNLL  701.963  TstNLL   69.886  TrnA 0.613  TstA 0.612\n",
      "Update    11  TrnNLL  3668.834  TstNLL  360.648  TrnA 0.508  TstA 0.552\n",
      "Update    16  TrnNLL  3341.465  TstNLL  325.427  TrnA 0.519  TstA 0.552\n",
      "Update    21  TrnNLL  2718.659  TstNLL  263.318  TrnA 0.530  TstA 0.578\n",
      "Update    26  TrnNLL  2238.830  TstNLL  221.453  TrnA 0.565  TstA 0.595\n",
      "Update    31  TrnNLL  1330.688  TstNLL  138.514  TrnA 0.664  TstA 0.655\n",
      "Update    36  TrnNLL  1196.192  TstNLL  127.621  TrnA 0.675  TstA 0.681\n",
      "Update    41  TrnNLL  531.950  TstNLL   73.362  TrnA 0.804  TstA 0.767\n",
      "Update    46  TrnNLL  461.110  TstNLL   54.713  TrnA 0.833  TstA 0.810\n",
      "Update    51  TrnNLL  444.985  TstNLL   52.886  TrnA 0.840  TstA 0.810\n",
      "Update    56  TrnNLL  430.765  TstNLL   53.224  TrnA 0.837  TstA 0.802\n",
      "Update    61  TrnNLL  417.186  TstNLL   50.156  TrnA 0.845  TstA 0.810\n",
      "Update    66  TrnNLL  529.548  TstNLL   61.614  TrnA 0.814  TstA 0.793\n",
      "Update    71  TrnNLL  526.456  TstNLL   61.771  TrnA 0.813  TstA 0.793\n",
      "Update    76  TrnNLL  462.520  TstNLL   50.082  TrnA 0.832  TstA 0.819\n",
      "Update    81  TrnNLL  437.929  TstNLL   48.335  TrnA 0.836  TstA 0.810\n",
      "Update    86  TrnNLL  423.612  TstNLL   45.355  TrnA 0.839  TstA 0.819\n",
      "Update    91  TrnNLL  457.794  TstNLL   49.002  TrnA 0.830  TstA 0.819\n",
      "Update    96  TrnNLL  451.998  TstNLL   49.449  TrnA 0.834  TstA 0.810\n",
      "Update   101  TrnNLL  407.183  TstNLL   49.715  TrnA 0.841  TstA 0.819\n",
      "Update   106  TrnNLL  395.977  TstNLL   50.149  TrnA 0.863  TstA 0.845\n",
      "Update   111  TrnNLL  439.619  TstNLL   51.707  TrnA 0.832  TstA 0.784\n",
      "Update   116  TrnNLL  423.910  TstNLL   50.608  TrnA 0.836  TstA 0.810\n",
      "Update   121  TrnNLL  435.833  TstNLL   52.176  TrnA 0.828  TstA 0.784\n",
      "Update   126  TrnNLL  445.506  TstNLL   54.501  TrnA 0.826  TstA 0.776\n",
      "Update   131  TrnNLL  414.541  TstNLL   51.924  TrnA 0.851  TstA 0.802\n",
      "Update   136  TrnNLL  442.494  TstNLL   54.657  TrnA 0.835  TstA 0.784\n",
      "Update   141  TrnNLL  495.511  TstNLL   58.412  TrnA 0.796  TstA 0.741\n",
      "Update   146  TrnNLL  531.751  TstNLL   62.137  TrnA 0.778  TstA 0.741\n",
      "Update   151  TrnNLL  536.381  TstNLL   63.038  TrnA 0.771  TstA 0.724\n",
      "Update   156  TrnNLL  477.488  TstNLL   59.145  TrnA 0.801  TstA 0.759\n",
      "Update   161  TrnNLL  472.235  TstNLL   58.686  TrnA 0.820  TstA 0.741\n",
      "Update   166  TrnNLL  458.674  TstNLL   57.365  TrnA 0.821  TstA 0.767\n",
      "Update   171  TrnNLL  442.606  TstNLL   56.852  TrnA 0.842  TstA 0.767\n",
      "Update   176  TrnNLL  436.634  TstNLL   59.247  TrnA 0.855  TstA 0.793\n",
      "Update   181  TrnNLL  422.293  TstNLL   55.117  TrnA 0.857  TstA 0.784\n",
      "Update   186  TrnNLL  501.882  TstNLL   64.034  TrnA 0.788  TstA 0.698\n",
      "Update   191  TrnNLL  427.717  TstNLL   56.288  TrnA 0.843  TstA 0.802\n",
      "Update   196  TrnNLL  418.414  TstNLL   59.991  TrnA 0.862  TstA 0.802\n",
      "Update   201  TrnNLL  411.116  TstNLL   60.043  TrnA 0.866  TstA 0.793\n",
      "Update   206  TrnNLL  422.524  TstNLL   61.124  TrnA 0.864  TstA 0.802\n",
      "Update   211  TrnNLL  430.650  TstNLL   61.887  TrnA 0.862  TstA 0.793\n",
      "Update   216  TrnNLL  452.558  TstNLL   62.071  TrnA 0.827  TstA 0.741\n",
      "Update   221  TrnNLL  441.230  TstNLL   59.538  TrnA 0.851  TstA 0.793\n",
      "Update   226  TrnNLL  416.790  TstNLL   61.361  TrnA 0.862  TstA 0.784\n",
      "Update   231  TrnNLL  498.288  TstNLL   77.276  TrnA 0.795  TstA 0.664\n",
      "Update   236  TrnNLL  423.209  TstNLL   64.990  TrnA 0.850  TstA 0.707\n",
      "Update   241  TrnNLL  411.741  TstNLL   63.537  TrnA 0.848  TstA 0.707\n",
      "Update   246  TrnNLL  403.478  TstNLL   61.111  TrnA 0.862  TstA 0.741\n",
      "Update   251  TrnNLL  418.274  TstNLL   63.414  TrnA 0.868  TstA 0.810\n",
      "Update   256  TrnNLL  408.545  TstNLL   57.912  TrnA 0.866  TstA 0.759\n",
      "Update   261  TrnNLL  419.831  TstNLL   57.265  TrnA 0.873  TstA 0.784\n",
      "Update   266  TrnNLL  414.842  TstNLL   55.770  TrnA 0.858  TstA 0.784\n",
      "Update   271  TrnNLL  487.891  TstNLL   67.098  TrnA 0.830  TstA 0.741\n",
      "Update   276  TrnNLL  398.856  TstNLL   54.839  TrnA 0.879  TstA 0.802\n",
      "Update   281  TrnNLL  390.396  TstNLL   51.050  TrnA 0.881  TstA 0.853\n",
      "Update   286  TrnNLL  394.702  TstNLL   51.752  TrnA 0.884  TstA 0.853\n",
      "Update   291  TrnNLL  373.567  TstNLL   48.619  TrnA 0.899  TstA 0.862\n",
      "Update   296  TrnNLL  420.230  TstNLL   54.917  TrnA 0.863  TstA 0.810\n",
      "Update   301  TrnNLL  711.429  TstNLL   92.800  TrnA 0.695  TstA 0.638\n",
      "Update   306  TrnNLL  429.437  TstNLL   56.632  TrnA 0.822  TstA 0.741\n",
      "Update   311  TrnNLL  446.507  TstNLL   58.014  TrnA 0.809  TstA 0.741\n",
      "Update   316  TrnNLL  408.496  TstNLL   53.731  TrnA 0.841  TstA 0.767\n",
      "Update   321  TrnNLL  389.156  TstNLL   50.635  TrnA 0.881  TstA 0.784\n",
      "Update   326  TrnNLL  393.000  TstNLL   49.872  TrnA 0.878  TstA 0.828\n",
      "Update   331  TrnNLL  377.275  TstNLL   50.143  TrnA 0.879  TstA 0.784\n",
      "Update   336  TrnNLL  392.470  TstNLL   51.879  TrnA 0.861  TstA 0.741\n",
      "Update   341  TrnNLL  382.464  TstNLL   49.990  TrnA 0.867  TstA 0.759\n",
      "Update   346  TrnNLL  360.784  TstNLL   46.585  TrnA 0.902  TstA 0.845\n",
      "Update   351  TrnNLL  374.608  TstNLL   48.963  TrnA 0.892  TstA 0.819\n",
      "Update   356  TrnNLL  361.948  TstNLL   48.621  TrnA 0.903  TstA 0.853\n",
      "Update   361  TrnNLL  361.200  TstNLL   48.775  TrnA 0.906  TstA 0.836\n",
      "Update   366  TrnNLL  368.675  TstNLL   49.415  TrnA 0.897  TstA 0.819\n",
      "Update   371  TrnNLL  363.468  TstNLL   49.245  TrnA 0.881  TstA 0.793\n",
      "Update   376  TrnNLL  363.328  TstNLL   49.117  TrnA 0.889  TstA 0.810\n",
      "Update   381  TrnNLL  353.591  TstNLL   45.652  TrnA 0.915  TstA 0.862\n",
      "Update   386  TrnNLL  354.570  TstNLL   45.584  TrnA 0.914  TstA 0.862\n",
      "Update   391  TrnNLL  359.715  TstNLL   46.982  TrnA 0.886  TstA 0.845\n",
      "Update   396  TrnNLL  372.005  TstNLL   48.766  TrnA 0.895  TstA 0.845\n",
      "Update   401  TrnNLL  354.524  TstNLL   47.236  TrnA 0.914  TstA 0.853\n",
      "Update   406  TrnNLL  367.217  TstNLL   48.050  TrnA 0.909  TstA 0.862\n",
      "Update   411  TrnNLL  368.306  TstNLL   48.232  TrnA 0.906  TstA 0.853\n",
      "Update   416  TrnNLL  385.752  TstNLL   50.261  TrnA 0.890  TstA 0.819\n",
      "Update   421  TrnNLL  379.516  TstNLL   48.649  TrnA 0.906  TstA 0.871\n",
      "Update   426  TrnNLL  365.766  TstNLL   47.206  TrnA 0.911  TstA 0.897\n",
      "Update   431  TrnNLL  365.853  TstNLL   47.564  TrnA 0.909  TstA 0.836\n",
      "Update   436  TrnNLL  366.967  TstNLL   48.129  TrnA 0.904  TstA 0.836\n",
      "Update   441  TrnNLL  366.472  TstNLL   49.264  TrnA 0.908  TstA 0.845\n",
      "Update   446  TrnNLL  371.980  TstNLL   50.251  TrnA 0.912  TstA 0.836\n",
      "Update   451  TrnNLL  375.349  TstNLL   50.528  TrnA 0.912  TstA 0.853\n",
      "Update   456  TrnNLL  380.294  TstNLL   52.286  TrnA 0.913  TstA 0.853\n",
      "Update   461  TrnNLL  386.369  TstNLL   52.677  TrnA 0.888  TstA 0.819\n",
      "Update   466  TrnNLL  375.368  TstNLL   50.203  TrnA 0.912  TstA 0.828\n",
      "Update   471  TrnNLL  688.865  TstNLL   75.286  TrnA 0.698  TstA 0.698\n",
      "Update   476  TrnNLL  526.645  TstNLL   60.044  TrnA 0.800  TstA 0.819\n",
      "Update   481  TrnNLL  407.740  TstNLL   50.231  TrnA 0.888  TstA 0.888\n",
      "Update   486  TrnNLL  407.531  TstNLL   49.883  TrnA 0.888  TstA 0.897\n",
      "Update   491  TrnNLL  464.948  TstNLL   55.018  TrnA 0.823  TstA 0.819\n",
      "Update   496  TrnNLL  365.552  TstNLL   47.155  TrnA 0.923  TstA 0.888\n",
      "Update   501  TrnNLL  397.161  TstNLL   49.504  TrnA 0.901  TstA 0.888\n",
      "Update   506  TrnNLL  426.695  TstNLL   52.084  TrnA 0.859  TstA 0.862\n",
      "Update   511  TrnNLL  402.020  TstNLL   50.396  TrnA 0.906  TstA 0.879\n",
      "Update   516  TrnNLL  392.777  TstNLL   48.959  TrnA 0.913  TstA 0.879\n",
      "Update   521  TrnNLL  391.592  TstNLL   49.744  TrnA 0.904  TstA 0.879\n",
      "Update   526  TrnNLL  402.253  TstNLL   49.243  TrnA 0.920  TstA 0.905\n",
      "Update   531  TrnNLL  388.023  TstNLL   47.329  TrnA 0.917  TstA 0.922\n",
      "Update   536  TrnNLL  406.627  TstNLL   48.837  TrnA 0.892  TstA 0.862\n",
      "Update   541  TrnNLL  401.615  TstNLL   48.259  TrnA 0.908  TstA 0.922\n",
      "Update   546  TrnNLL  402.021  TstNLL   49.676  TrnA 0.918  TstA 0.871\n",
      "Update   551  TrnNLL  399.164  TstNLL   48.425  TrnA 0.915  TstA 0.888\n",
      "Update   556  TrnNLL  386.495  TstNLL   46.904  TrnA 0.922  TstA 0.914\n",
      "Update   561  TrnNLL  468.820  TstNLL   56.333  TrnA 0.796  TstA 0.810\n",
      "Update   566  TrnNLL  482.298  TstNLL   56.868  TrnA 0.763  TstA 0.784\n",
      "Update   571  TrnNLL  431.738  TstNLL   50.313  TrnA 0.857  TstA 0.853\n",
      "Update   576  TrnNLL  404.378  TstNLL   46.395  TrnA 0.912  TstA 0.897\n",
      "Update   581  TrnNLL  399.222  TstNLL   46.267  TrnA 0.913  TstA 0.931\n",
      "Update   586  TrnNLL  404.319  TstNLL   46.907  TrnA 0.903  TstA 0.888\n",
      "Update   591  TrnNLL  408.753  TstNLL   48.232  TrnA 0.896  TstA 0.905\n",
      "Update   596  TrnNLL  419.083  TstNLL   50.005  TrnA 0.887  TstA 0.871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update   601  TrnNLL  444.619  TstNLL   51.830  TrnA 0.818  TstA 0.802\n",
      "Update   606  TrnNLL  382.082  TstNLL   46.232  TrnA 0.943  TstA 0.905\n",
      "Update   611  TrnNLL  399.922  TstNLL   49.190  TrnA 0.902  TstA 0.853\n",
      "Update   616  TrnNLL  404.549  TstNLL   48.684  TrnA 0.929  TstA 0.914\n",
      "Update   621  TrnNLL  409.620  TstNLL   49.329  TrnA 0.923  TstA 0.888\n",
      "Update   626  TrnNLL  440.259  TstNLL   52.009  TrnA 0.832  TstA 0.853\n",
      "Update   631  TrnNLL  410.997  TstNLL   49.097  TrnA 0.907  TstA 0.897\n",
      "Update   636  TrnNLL  409.382  TstNLL   49.343  TrnA 0.937  TstA 0.922\n",
      "Update   641  TrnNLL  418.111  TstNLL   51.288  TrnA 0.935  TstA 0.914\n",
      "Update   646  TrnNLL  417.563  TstNLL   51.450  TrnA 0.929  TstA 0.905\n",
      "Update   651  TrnNLL  450.265  TstNLL   55.909  TrnA 0.858  TstA 0.767\n",
      "Update   656  TrnNLL  456.221  TstNLL   56.708  TrnA 0.858  TstA 0.784\n",
      "Update   661  TrnNLL  432.423  TstNLL   53.961  TrnA 0.923  TstA 0.879\n",
      "Update   666  TrnNLL  435.494  TstNLL   54.556  TrnA 0.940  TstA 0.888\n",
      "Update   671  TrnNLL  431.666  TstNLL   54.670  TrnA 0.928  TstA 0.828\n",
      "Update   676  TrnNLL  427.991  TstNLL   53.481  TrnA 0.947  TstA 0.914\n",
      "Update   681  TrnNLL  444.522  TstNLL   54.419  TrnA 0.911  TstA 0.871\n",
      "Update   686  TrnNLL  473.121  TstNLL   57.588  TrnA 0.874  TstA 0.802\n",
      "Update   691  TrnNLL  445.407  TstNLL   53.970  TrnA 0.911  TstA 0.845\n",
      "Update   696  TrnNLL  435.861  TstNLL   54.782  TrnA 0.937  TstA 0.879\n",
      "Update   701  TrnNLL  463.865  TstNLL   56.212  TrnA 0.870  TstA 0.836\n",
      "Update   706  TrnNLL  438.497  TstNLL   53.128  TrnA 0.940  TstA 0.905\n",
      "Update   711  TrnNLL  466.347  TstNLL   54.773  TrnA 0.897  TstA 0.888\n",
      "Update   716  TrnNLL  466.877  TstNLL   55.880  TrnA 0.919  TstA 0.888\n",
      "Update   721  TrnNLL  457.504  TstNLL   54.208  TrnA 0.926  TstA 0.879\n",
      "Update   726  TrnNLL  441.985  TstNLL   52.474  TrnA 0.944  TstA 0.922\n",
      "Update   731  TrnNLL  443.096  TstNLL   53.372  TrnA 0.914  TstA 0.862\n",
      "Update   736  TrnNLL  433.589  TstNLL   52.996  TrnA 0.951  TstA 0.897\n",
      "Update   741  TrnNLL  456.840  TstNLL   55.908  TrnA 0.947  TstA 0.914\n",
      "Update   746  TrnNLL  465.005  TstNLL   57.433  TrnA 0.930  TstA 0.905\n",
      "Update   751  TrnNLL  457.497  TstNLL   56.349  TrnA 0.933  TstA 0.897\n",
      "Update   756  TrnNLL  470.561  TstNLL   57.775  TrnA 0.888  TstA 0.828\n",
      "Update   761  TrnNLL  462.860  TstNLL   56.799  TrnA 0.935  TstA 0.879\n",
      "Update   766  TrnNLL  467.072  TstNLL   56.658  TrnA 0.900  TstA 0.862\n",
      "Update   771  TrnNLL  492.538  TstNLL   59.847  TrnA 0.852  TstA 0.819\n",
      "Update   776  TrnNLL  499.362  TstNLL   60.212  TrnA 0.851  TstA 0.810\n",
      "Update   781  TrnNLL  504.608  TstNLL   61.117  TrnA 0.818  TstA 0.759\n",
      "Update   786  TrnNLL  482.565  TstNLL   58.806  TrnA 0.891  TstA 0.828\n",
      "Update   791  TrnNLL  475.172  TstNLL   57.582  TrnA 0.929  TstA 0.905\n",
      "Update   796  TrnNLL  500.139  TstNLL   61.374  TrnA 0.822  TstA 0.767\n",
      "Update   801  TrnNLL  485.144  TstNLL   59.889  TrnA 0.845  TstA 0.810\n",
      "Update   806  TrnNLL  467.354  TstNLL   56.425  TrnA 0.934  TstA 0.905\n",
      "Update   811  TrnNLL  474.183  TstNLL   56.957  TrnA 0.914  TstA 0.914\n",
      "Update   816  TrnNLL  454.332  TstNLL   55.126  TrnA 0.943  TstA 0.888\n",
      "Update   821  TrnNLL  467.135  TstNLL   55.973  TrnA 0.924  TstA 0.879\n",
      "Update   826  TrnNLL  476.301  TstNLL   57.534  TrnA 0.908  TstA 0.862\n",
      "Update   831  TrnNLL  491.553  TstNLL   58.615  TrnA 0.839  TstA 0.802\n",
      "Update   836  TrnNLL  472.658  TstNLL   55.072  TrnA 0.905  TstA 0.940\n",
      "Update   841  TrnNLL  502.546  TstNLL   59.026  TrnA 0.798  TstA 0.784\n",
      "Update   846  TrnNLL  506.213  TstNLL   59.656  TrnA 0.774  TstA 0.741\n",
      "Update   851  TrnNLL  500.973  TstNLL   60.161  TrnA 0.786  TstA 0.767\n",
      "Update   856  TrnNLL  501.801  TstNLL   59.085  TrnA 0.859  TstA 0.810\n",
      "Update   861  TrnNLL  482.946  TstNLL   56.464  TrnA 0.896  TstA 0.931\n",
      "Update   866  TrnNLL  481.841  TstNLL   55.957  TrnA 0.911  TstA 0.914\n",
      "Update   871  TrnNLL  482.255  TstNLL   55.740  TrnA 0.913  TstA 0.931\n",
      "Update   876  TrnNLL  479.217  TstNLL   55.600  TrnA 0.936  TstA 0.905\n",
      "Update   881  TrnNLL  487.539  TstNLL   56.296  TrnA 0.914  TstA 0.888\n",
      "Update   886  TrnNLL  481.421  TstNLL   54.646  TrnA 0.927  TstA 0.931\n",
      "Update   891  TrnNLL  485.008  TstNLL   55.193  TrnA 0.914  TstA 0.931\n",
      "Update   896  TrnNLL  486.556  TstNLL   55.288  TrnA 0.877  TstA 0.905\n",
      "Update   901  TrnNLL  515.056  TstNLL   53.774  TrnA 0.884  TstA 0.922\n",
      "Update   906  TrnNLL  529.080  TstNLL   55.545  TrnA 0.908  TstA 0.940\n",
      "Update   911  TrnNLL  537.115  TstNLL   56.389  TrnA 0.904  TstA 0.940\n",
      "Update   916  TrnNLL  538.691  TstNLL   56.030  TrnA 0.907  TstA 0.931\n",
      "Update   921  TrnNLL  532.367  TstNLL   56.982  TrnA 0.918  TstA 0.922\n",
      "Update   926  TrnNLL  533.639  TstNLL   56.857  TrnA 0.886  TstA 0.922\n",
      "Update   931  TrnNLL  524.533  TstNLL   55.826  TrnA 0.914  TstA 0.931\n",
      "Update   936  TrnNLL  535.427  TstNLL   56.831  TrnA 0.905  TstA 0.914\n",
      "Update   941  TrnNLL  529.544  TstNLL   56.814  TrnA 0.911  TstA 0.905\n",
      "Update   946  TrnNLL  545.912  TstNLL   58.745  TrnA 0.848  TstA 0.845\n",
      "Update   951  TrnNLL  508.554  TstNLL   54.494  TrnA 0.938  TstA 0.940\n",
      "Update   956  TrnNLL  532.859  TstNLL   56.834  TrnA 0.912  TstA 0.922\n",
      "Update   961  TrnNLL  528.813  TstNLL   57.049  TrnA 0.922  TstA 0.922\n",
      "Update   966  TrnNLL  537.251  TstNLL   58.905  TrnA 0.862  TstA 0.853\n",
      "Update   971  TrnNLL  541.635  TstNLL   58.872  TrnA 0.853  TstA 0.819\n",
      "Update   976  TrnNLL  560.036  TstNLL   60.629  TrnA 0.801  TstA 0.802\n",
      "Update   981  TrnNLL  553.851  TstNLL   60.463  TrnA 0.817  TstA 0.828\n",
      "Update   986  TrnNLL  562.189  TstNLL   61.453  TrnA 0.844  TstA 0.836\n",
      "Update   991  TrnNLL  553.464  TstNLL   60.273  TrnA 0.918  TstA 0.897\n",
      "Update   996  TrnNLL  541.024  TstNLL   59.516  TrnA 0.880  TstA 0.897\n",
      "Update  1001  TrnNLL  538.324  TstNLL   59.089  TrnA 0.923  TstA 0.888\n",
      "Update  1006  TrnNLL  570.058  TstNLL   62.131  TrnA 0.763  TstA 0.759\n",
      "Update  1011  TrnNLL  554.679  TstNLL   60.676  TrnA 0.866  TstA 0.871\n",
      "Update  1016  TrnNLL  552.927  TstNLL   61.099  TrnA 0.835  TstA 0.836\n",
      "Update  1021  TrnNLL  589.958  TstNLL   65.026  TrnA 0.707  TstA 0.690\n",
      "Update  1026  TrnNLL  585.336  TstNLL   64.914  TrnA 0.704  TstA 0.664\n",
      "Update  1031  TrnNLL  554.987  TstNLL   61.296  TrnA 0.808  TstA 0.793\n",
      "Update  1036  TrnNLL  555.390  TstNLL   61.718  TrnA 0.812  TstA 0.767\n",
      "Update  1041  TrnNLL  575.246  TstNLL   62.828  TrnA 0.899  TstA 0.871\n",
      "Update  1046  TrnNLL  580.068  TstNLL   63.292  TrnA 0.901  TstA 0.897\n",
      "Update  1051  TrnNLL  554.177  TstNLL   63.146  TrnA 0.871  TstA 0.853\n",
      "Update  1056  TrnNLL  610.737  TstNLL   68.234  TrnA 0.704  TstA 0.698\n",
      "Update  1061  TrnNLL  530.257  TstNLL   62.479  TrnA 0.866  TstA 0.819\n",
      "Update  1066  TrnNLL  500.275  TstNLL   59.047  TrnA 0.921  TstA 0.914\n",
      "Update  1071  TrnNLL  511.500  TstNLL   59.956  TrnA 0.914  TstA 0.897\n",
      "Update  1076  TrnNLL  514.297  TstNLL   59.817  TrnA 0.931  TstA 0.914\n",
      "Motorcycle: \n",
      "['patches', 'lock', 'cable', 'race', 'donuts', 'digest', 'ftp', 'computers', 'joust', 'denizens']\n",
      "Car: \n",
      "['requests', 'participants', 'boole', 'largely', 'invader', 'saturn', 'ky', 'westminster', 'payments', 'mart']\n",
      "Bad: \n",
      "['component', 'increased', 'tongue', 'throughout', 'bulletin', 'clear', 'russian', 'agreement', 'program', 'plastic']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "kSEED = 1735\n",
    "kBIAS = \"BIAS_CONSTANT\"\n",
    "\n",
    "np.random.seed(kSEED)\n",
    "\n",
    "class Example:\n",
    "    \"\"\"\n",
    "    Class to represent a document example\n",
    "    \"\"\"\n",
    "    def __init__(self, label, words, vocab):\n",
    "        \"\"\"\n",
    "        Create a new example\n",
    "\n",
    "        :param label: The label (0 / 1) of the example\n",
    "        :param words: The words in a list of \"word:count\" format\n",
    "        :param vocab: The vocabulary to use as features (list)\n",
    "        \"\"\"\n",
    "        self.nonzero = {}\n",
    "        self.y = label\n",
    "        self.x = np.zeros(len(vocab))\n",
    "        for word, count in [x.split(\":\") for x in words]:\n",
    "            if word in vocab:\n",
    "                assert word != kBIAS, \"Bias can't actually appear in document\"\n",
    "                self.x[vocab.index(word)] += float(count)\n",
    "                self.nonzero[vocab.index(word)] = word\n",
    "        self.x[0] = 1\n",
    "\n",
    "def read_dataset(positive, negative, vocab, train_frac=0.9):\n",
    "    \"\"\"\n",
    "    Reads in a text dataset with a given vocabulary\n",
    "\n",
    "    :param positive: Positive examples\n",
    "    :param negative: Negative examples\n",
    "    :param vocab: A list of vocabulary words\n",
    "    :param test_frac: How much of the data should be reserved for test\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = [x.split(\"\\t\")[0] for x in open(vocab, 'r') if '\\t' in x]\n",
    "    assert vocab[0] == kBIAS, \\\n",
    "        \"First vocab word must be bias term (was %s)\" % vocab[0]\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for label, input in [(1, positive), (0, negative)]:\n",
    "        for line in open(input):\n",
    "            ex = Example(label, line.split(), vocab)\n",
    "            if np.random.random() <= train_frac:\n",
    "                train_set.append(ex)\n",
    "            else:\n",
    "                test_set.append(ex)\n",
    "\n",
    "    # Shuffle the data \n",
    "    np.random.shuffle(train_set)\n",
    "    np.random.shuffle(test_set)\n",
    "\n",
    "    return train_set, test_set, vocab\n",
    "\n",
    "\n",
    "pos_fname = \"positive\"\n",
    "neg_fname = \"negative\"\n",
    "voc_fname = \"vocab\"\n",
    "train_set, test_set, vocab = read_dataset(pos_fname, neg_fname, voc_fname)\n",
    "print(len(vocab))\n",
    "\n",
    "class LogReg:\n",
    "    def __init__(self, train_set, test_set, lam, eta=0.1):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "\n",
    "        :param train_set: A set of training examples\n",
    "        :param test_set: A set of test examples \n",
    "        :param lam: Regularization parameter\n",
    "        :param eta: The learning rate to use \n",
    "        \"\"\"\n",
    "        \n",
    "        # Store training and test sets \n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set \n",
    "        \n",
    "        # Initialize vector of weights to zero  \n",
    "        self.w = np.zeros_like(train_set[0].x)\n",
    "        \n",
    "        # Store regularization parameter and eta function \n",
    "        self.lam = lam\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Create dictionary for lazy-sparse regularization\n",
    "        self.last_update = defaultdict(int)\n",
    "\n",
    "        # Make sure regularization parameter is not negative \n",
    "        assert self.lam>= 0, \"Regularization parameter must be non-negative\"\n",
    "        \n",
    "        # Empty lists to store NLL and accuracy on train and test sets \n",
    "        self.train_nll = []\n",
    "        self.test_nll = []\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "      \n",
    "    def sigmoid(self,score, threshold=20.0):\n",
    "        \"\"\"\n",
    "        Prevent overflow of exp by capping activation at 20.\n",
    "        You do not need to change this function. \n",
    "\n",
    "        :param score: A real valued number to convert into a number between 0 and 1\n",
    "        \"\"\"\n",
    "\n",
    "        # if score > threshold, cap value at score \n",
    "        if abs(score) > threshold:\n",
    "            score = threshold * np.sign(score)\n",
    "\n",
    "        return 1.0 / (1.0 + np.exp(-score)) \n",
    "\n",
    "    def compute_progress(self, examples):\n",
    "        \"\"\"\n",
    "        Given a set of examples, compute the NLL and accuracy\n",
    "        You shouldn't need to change this function. \n",
    "\n",
    "        :param examples: The dataset to score\n",
    "        :return: A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "\n",
    "        NLL = 0.0\n",
    "        num_correct = 0\n",
    "        for ex in examples:\n",
    "            # compute prob prediction\n",
    "            p = self.sigmoid(self.w.dot(ex.x))\n",
    "            # update negative log likelihood\n",
    "            NLL = NLL - np.log(p) if ex.y==1 else NLL - np.log(1.0-p)\n",
    "            # update number correct \n",
    "            num_correct += 1 if np.floor(p+.5)==ex.y else 0\n",
    "\n",
    "        return NLL, float(num_correct) / float(len(examples))\n",
    "    \n",
    "    def train(self, num_epochs=1, isVerbose=False, report_step=5):\n",
    "        \"\"\"\n",
    "        Train the logistic regression classifier on the training data \n",
    "\n",
    "        :param num_epochs: number of full passes over data to perform \n",
    "        :param isVerbose: boolean indicating whether to print progress\n",
    "        :param report_step: how many iterations between recording progress\n",
    "        \"\"\"\n",
    "        iteration = 0\n",
    "        # Perform an epoch \n",
    "        for pp in range(num_epochs):\n",
    "            # shuffle the data  \n",
    "            np.random.shuffle(self.train_set)\n",
    "            # loop over each training example\n",
    "            for ex in self.train_set:\n",
    "                # perform SGD update of weights\n",
    "                iteration += 1\n",
    "                self.sgd_update(ex, iteration)\n",
    "                # record progress \n",
    "                if iteration % report_step == 1:\n",
    "                    train_nll, train_acc = self.compute_progress(self.train_set)\n",
    "                    test_nll, test_acc = self.compute_progress(self.test_set)\n",
    "                    self.train_nll.append(train_nll)\n",
    "                    self.test_nll.append(test_nll)\n",
    "                    self.train_acc.append(train_acc)\n",
    "                    self.test_acc.append(test_acc)\n",
    "                    \n",
    "                    if isVerbose:\n",
    "                        print(\"Update {: 5d}  TrnNLL {: 8.3f}  TstNLL {: 8.3f}  TrnA {:.3f}  TstA {:.3f}\"\n",
    "                             .format(iteration, train_nll, test_nll, train_acc, test_acc))\n",
    "                \n",
    "    \n",
    "    def sgd_update(self, train_example, iteration, use_tfidf=False):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update to improve the log likelihood.\n",
    "        :param train_example: The example to take the gradient with respect to\n",
    "        :param iteration: The current iteration (an integer)\n",
    "        :param use_tfidf: A boolean to switch between the raw data and the tfidf representation\n",
    "        :return: Return the new value of the regression coefficients\n",
    "        \"\"\"\n",
    "        \n",
    "        idx = list(train_example.nonzero.keys())\n",
    "        idx.append(0)\n",
    "        \n",
    "        x    =  train_example.x\n",
    "        y    =  train_example.y\n",
    "\n",
    "        ebiasx = math.exp(self.w[idx].dot(x[idx]))\n",
    "        p = ebiasx/float(1+ebiasx)\n",
    "        self.w[idx] = self.w[idx] + ((self.eta) * (y - p) * x[idx])\n",
    "        biasunreg = self.w[0] \n",
    "        \n",
    "        for i in range(0, len(self.w)):\n",
    "            self.last_update[i] += 1\n",
    "        regular = np.power((1 - (2 * self.eta * self.lam)),((list(self.last_update.values()))))\n",
    "        self.w[idx] = self.w[idx] * regular[idx]\n",
    "        \n",
    "        self.w[0] = biasunreg\n",
    "\n",
    "\n",
    "# %run -i tests.py \"part A\"\n",
    "# %run -i tests.py \"part B\"\n",
    "    \n",
    "    def goodbbwords(self):\n",
    "        return np.argsort(self.w)[-10:]\n",
    "    def goodhwords(self):\n",
    "        return np.argsort(self.w)[:10]\n",
    "    def badwords(self):\n",
    "        return np.argsort(abs(self.w))[:10] #Getting the zeros\n",
    "\n",
    "def results(lr):\n",
    "    print (\"Motorcycle: \")\n",
    "    baseball = lr.goodbbwords()\n",
    "    print([vocab[i] for i in baseball])\n",
    "    print(\"Car: \")\n",
    "    hockey = lr.goodhwords()\n",
    "    print([vocab[i] for i in hockey])\n",
    "    print (\"Bad: \")\n",
    "    bad = lr.badwords()\n",
    "    print([vocab[i] for i in bad])\n",
    "\n",
    "\n",
    "lr = LogReg(train_set, test_set, lam=0.006, eta=0.1)\n",
    "lr.train(isVerbose=True)\n",
    "results(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Implementing SGD with Lazy Sparse Regularization\n",
    "***\n",
    "\n",
    "We've given you a class `LogReg` below which will train a logistic regression classifier to predict whether a document is talking about automobiles or motorcycles. \n",
    "\n",
    "**Part A**: In this problem you will modify the `sgd_update` function to perform **unregularized** stochastic gradient descent updates of the weights. Note that you should only update the weights for **non-zero** features, i.e. weights associated with words that appear in the current training example. The code below this cell demonstrates how to instantiate the class and train the classifier.   \n",
    "\n",
    "We've also given you unit tests in the next cell based on the simple example worked out in  the Lecture 4 in-class notebook.  At first your code will fail both of them. When your code is working you should pass tests called `test_unreg` and `test_learnrate`.  Do not move on to **Part A** until your code passes both of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit tests are located in the script `tests.py` in this directory.  Execute the following cell to call the script and run the tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: After your unregularized updates are working, modify the `sgd_update` function again to perform regularized updates using **Lazy Sparse Regularization**. Note that you should not regularize the bias weight. See the Lecture 4 in-class notebook for a refresher on LSR. **Note**: After implementing LSR, your code should still pass the unit tests for **Part A** when `lam = 0`. \n",
    "\n",
    "We've given you a third unit test in the next cell called `test_reg` based on the simple example of LSR worked out in  the Lecture 4 in-class notebook.  Do not move on to **Problem 3** until your code passes the test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 3: Hyperparameter Tuning \n",
    "***\n",
    "\n",
    "**Part A**: Perform a systematic study of the effect of the regularization parameter on the accuracy of your classifier on the test set.  Which choice of `lam` seems to do the best?  Justify your conclusion with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: For the value of `lam` chosen in **Part A** perform a systematic study of the choice of learning rate on the speed of convergence SGD.  Which learning rate seems to give the fastest convergence?  Justify your conclusion with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 4: Identifying Predictive and Non-Predictive Words \n",
    "***\n",
    "\n",
    "**Part A**: Find the top 10 words that are the best predictors for each class.  Explain mathematically how you identified them and show any code that you used to find them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Find the 10 words that are the worst predictors for class.  Explain mathematically how you identified them and show any code that you used to find them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
